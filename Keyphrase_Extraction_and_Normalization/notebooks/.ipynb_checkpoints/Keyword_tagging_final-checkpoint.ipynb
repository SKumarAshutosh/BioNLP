{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\Pytorchgpu\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\HP\\anaconda3\\envs\\Pytorchgpu\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\Users\\HP\\anaconda3\\envs\\Pytorchgpu\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re,string\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "# import fasttext\n",
    "import spacy\n",
    "\n",
    "\n",
    "import nltk\n",
    "import networkx as nx\n",
    "from nltk.corpus import wordnet,stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import WordNetLemmatizer, pos_tag, word_tokenize\n",
    "\n",
    "stop_words=stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from keras.layers import Activation, Embedding, Reshape,Concatenate\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import skipgrams, make_sampling_table\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from typing import NamedTuple, List\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "# import networkx as nx\n",
    "from itertools import combinations\n",
    "from collections import defaultdict  \n",
    "import abc\n",
    "from collections import Counter    \n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>glucose</th>\n",
       "      <th>college</th>\n",
       "      <th>country</th>\n",
       "      <th>grantID</th>\n",
       "      <th>Disease</th>\n",
       "      <th>center_copy_txt</th>\n",
       "      <th>Publication_Type_copy_txt</th>\n",
       "      <th>mesh_keyword_identified</th>\n",
       "      <th>organization_type_1st_level</th>\n",
       "      <th>mesh_term_qualified_desc</th>\n",
       "      <th>...</th>\n",
       "      <th>hospital_copy_txt</th>\n",
       "      <th>college_copy_txt</th>\n",
       "      <th>Journal_Title</th>\n",
       "      <th>Journal_Title_copy_txt</th>\n",
       "      <th>organization_name_1st_level_copy_txt</th>\n",
       "      <th>country_name_copy_txt</th>\n",
       "      <th>mesh_term_descriptor_name_copy_txt</th>\n",
       "      <th>category</th>\n",
       "      <th>PMID</th>\n",
       "      <th>tags_copy_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-08-06T01:24:45Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>journal article</td>\n",
       "      <td>ciprofloxacin,cipro,staphylococcus,vancomycin,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Foodborne Pathogens and Disease</td>\n",
       "      <td>Foodborne Pathogens and Disease</td>\n",
       "      <td>NaN</td>\n",
       "      <td>republic of,republic of,republic of,republic o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chemicals and Drugs,Organisms,Phenomena and Pr...</td>\n",
       "      <td>33769832.0</td>\n",
       "      <td>2010-2018,isolates methicillin,chicken carcass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-08-06T01:24:46Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>diabetes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>journal article,research support\\, non-u.s. gov't</td>\n",
       "      <td>NaN</td>\n",
       "      <td>university,university,university,university</td>\n",
       "      <td>diabetes mellitus\\, type 2,glucagon-like pepti...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Journal of Medicinal Chemistry</td>\n",
       "      <td>Journal of Medicinal Chemistry</td>\n",
       "      <td>sungkyunkwan university,sungkyunkwan universit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>animals,diabetes mellitus\\, type 2,glucagon-li...</td>\n",
       "      <td>Organisms,Diseases,Chemicals and Drugs,Phenome...</td>\n",
       "      <td>33769827.0</td>\n",
       "      <td>free fatty acid,pancreatic cell,glucagon,fatty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-08-06T01:24:47Z</td>\n",
       "      <td>south carolina college of pharmacy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>memory dysfunction,neurodegenerative diseases,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>journal article,research support\\, non-u.s. gov't</td>\n",
       "      <td>NaN</td>\n",
       "      <td>university,university,university,university,un...</td>\n",
       "      <td>alzheimer disease,coumaric acids,histone deace...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>south carolina college of pharmacy</td>\n",
       "      <td>Journal of Medicinal Chemistry</td>\n",
       "      <td>Journal of Medicinal Chemistry</td>\n",
       "      <td>julius maximilian university of w√ºrzburg,medic...</td>\n",
       "      <td>germany,united states,germany,italy,germany,ge...</td>\n",
       "      <td>alzheimer disease,animals,catalytic domain,cel...</td>\n",
       "      <td>Diseases,Psychiatry and Psychology,Organisms,P...</td>\n",
       "      <td>33769811.0</td>\n",
       "      <td>melatonin ferulic acid,dpph radical scavenging...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-08-06T01:24:51Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States,United States</td>\n",
       "      <td>S10 OD023579,R01 HL140468</td>\n",
       "      <td>stroke,diastole,heart failure,renal dysfunction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>journal article,research support\\, n.i.h.\\, ex...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>university,university,university,university,un...</td>\n",
       "      <td>adrenergic beta-antagonists,autoantigens,bioma...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American Journal of Physiology - Heart and Cir...</td>\n",
       "      <td>American Journal of Physiology - Heart and Cir...</td>\n",
       "      <td>university of miami leonard m. miller school o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adrenergic beta-antagonists,animals,autoantige...</td>\n",
       "      <td>Chemicals and Drugs,Organisms,Analytical\\, Dia...</td>\n",
       "      <td>33769915.0</td>\n",
       "      <td>col4a3-/- alport mouse,urine albumin,carvedilo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-08-06T01:24:53Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PIMS Code:OXX01915</td>\n",
       "      <td>mortality</td>\n",
       "      <td>nagaland centre,nagaland centre,nagaland centre</td>\n",
       "      <td>journal article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>center,center,center</td>\n",
       "      <td>chickens</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tropical Animal Health and Production</td>\n",
       "      <td>Tropical Animal Health and Production</td>\n",
       "      <td>nagaland centre,nagaland centre,nagaland centre</td>\n",
       "      <td>india,india,india,india</td>\n",
       "      <td>animals,chickens,female,india,male,ovum,tropic...</td>\n",
       "      <td>Organisms,Anatomy,Phenomena and Processes,Heal...</td>\n",
       "      <td>33770287.0</td>\n",
       "      <td>north eastern hill,chicken,yolk,srinidhi,germp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>2021-08-06T01:55:23Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neurological disorder,diabetes mellitus,fractu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>journal article,multicenter study,observationa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>university,university,university,hospital,hosp...</td>\n",
       "      <td>greece,malnutrition,mass screening,nutritional...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"p. and a. kyriakou\" children's hospital,\"p. a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nutrients</td>\n",
       "      <td>Nutrients</td>\n",
       "      <td>harokopio university,harokopio university,haro...</td>\n",
       "      <td>greece,greece,greece,greece,greece,greece,gree...</td>\n",
       "      <td>child,child\\, preschool,cross-sectional studie...</td>\n",
       "      <td>Analytical\\, Diagnostic and Therapeutic Techni...</td>\n",
       "      <td>33924630.0</td>\n",
       "      <td>pyms,clinical setting study,dietitian,whogc,su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>2021-08-06T01:55:23Z</td>\n",
       "      <td>jagiellonian university medical college</td>\n",
       "      <td>NaN</td>\n",
       "      <td>number NRSA 406754</td>\n",
       "      <td>hyporesponsiveness,rheumatoid arthritis,allerg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>journal article,randomized controlled trial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>university,university,university,university,un...</td>\n",
       "      <td>adaptation\\, physiological,athletic performanc...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jagiellonian university medical college</td>\n",
       "      <td>Nutrients</td>\n",
       "      <td>Nutrients</td>\n",
       "      <td>gda≈Ñsk university of physical education and sp...</td>\n",
       "      <td>poland,poland,poland,poland,poland,poland,pola...</td>\n",
       "      <td>adaptation\\, physiological,adult,athletic perf...</td>\n",
       "      <td>Phenomena and Processes,Anthropology\\, Educati...</td>\n",
       "      <td>33924645.0</td>\n",
       "      <td>leptin oncostatin,vitamin vegetable,interleuki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>2021-08-06T01:55:24Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fasting insulinemia,nonalcoholic fatty liver d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>journal article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>university,university,university,university,un...</td>\n",
       "      <td>arecaceae,berberine,berberis,coffea,diet\\, hig...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nutrients</td>\n",
       "      <td>Nutrients</td>\n",
       "      <td>university of naples ''federico ii'',universit...</td>\n",
       "      <td>italy,italy,italy,italy,italy,italy,italy,ital...</td>\n",
       "      <td>animals,arecaceae,berberine,berberis,coffea,di...</td>\n",
       "      <td>Organisms,Chemicals and Drugs,Phenomena and Pr...</td>\n",
       "      <td>33924725.0</td>\n",
       "      <td>aminotransferase hepatic,microbiota hepatic,be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>2021-08-06T01:55:25Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ANR-19-CE21-0005-01</td>\n",
       "      <td>obesity,chronic diseases,idiosyncratic error,m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>journal article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>university,university</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nutrients</td>\n",
       "      <td>Nutrients</td>\n",
       "      <td>universit√© de rennes 1,universit√© de fribourg</td>\n",
       "      <td>france,switzerland</td>\n",
       "      <td>adult,attitude to health,female,humans,male,nu...</td>\n",
       "      <td>Psychiatry and Psychology,Health Care,Organism...</td>\n",
       "      <td>33924784.0</td>\n",
       "      <td>coerciveness,coercive measure,social norm,nutr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>2021-08-06T01:55:26Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lumbar pain,hypersensitive,respiratory disease...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>journal article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>university,university,university,university,un...</td>\n",
       "      <td>annona,averrhoa,eugenia,fruit,garcinia,plant e...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nutrients</td>\n",
       "      <td>Nutrients</td>\n",
       "      <td>federal university of grande dourados,federal ...</td>\n",
       "      <td>brazil,brazil,brazil,brazil,brazil</td>\n",
       "      <td>animals,annona,averrhoa,brazil,eugenia,fruit,g...</td>\n",
       "      <td>Organisms,Analytical\\, Diagnostic and Therapeu...</td>\n",
       "      <td>33924791.0</td>\n",
       "      <td>creatinine urea,solid sugar,pulp,glutamic pyru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows √ó 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   glucose                                  college  \\\n",
       "0     2021-08-06T01:24:45Z                                      NaN   \n",
       "1     2021-08-06T01:24:46Z                                      NaN   \n",
       "2     2021-08-06T01:24:47Z       south carolina college of pharmacy   \n",
       "3     2021-08-06T01:24:51Z                                      NaN   \n",
       "4     2021-08-06T01:24:53Z                                      NaN   \n",
       "...                    ...                                      ...   \n",
       "4995  2021-08-06T01:55:23Z                                      NaN   \n",
       "4996  2021-08-06T01:55:23Z  jagiellonian university medical college   \n",
       "4997  2021-08-06T01:55:24Z                                      NaN   \n",
       "4998  2021-08-06T01:55:25Z                                      NaN   \n",
       "4999  2021-08-06T01:55:26Z                                      NaN   \n",
       "\n",
       "                          country                    grantID  \\\n",
       "0                             NaN                        NaN   \n",
       "1                             NaN                        NaN   \n",
       "2                             NaN                        NaN   \n",
       "3     United States,United States  S10 OD023579,R01 HL140468   \n",
       "4                             NaN         PIMS Code:OXX01915   \n",
       "...                           ...                        ...   \n",
       "4995                          NaN                        NaN   \n",
       "4996                          NaN         number NRSA 406754   \n",
       "4997                          NaN                        NaN   \n",
       "4998                          NaN        ANR-19-CE21-0005-01   \n",
       "4999                          NaN                        NaN   \n",
       "\n",
       "                                                Disease  \\\n",
       "0                                                   NaN   \n",
       "1                                              diabetes   \n",
       "2     memory dysfunction,neurodegenerative diseases,...   \n",
       "3       stroke,diastole,heart failure,renal dysfunction   \n",
       "4                                             mortality   \n",
       "...                                                 ...   \n",
       "4995  neurological disorder,diabetes mellitus,fractu...   \n",
       "4996  hyporesponsiveness,rheumatoid arthritis,allerg...   \n",
       "4997  fasting insulinemia,nonalcoholic fatty liver d...   \n",
       "4998  obesity,chronic diseases,idiosyncratic error,m...   \n",
       "4999  lumbar pain,hypersensitive,respiratory disease...   \n",
       "\n",
       "                                      center_copy_txt  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4     nagaland centre,nagaland centre,nagaland centre   \n",
       "...                                               ...   \n",
       "4995                                              NaN   \n",
       "4996                                              NaN   \n",
       "4997                                              NaN   \n",
       "4998                                              NaN   \n",
       "4999                                              NaN   \n",
       "\n",
       "                              Publication_Type_copy_txt  \\\n",
       "0                                       journal article   \n",
       "1     journal article,research support\\, non-u.s. gov't   \n",
       "2     journal article,research support\\, non-u.s. gov't   \n",
       "3     journal article,research support\\, n.i.h.\\, ex...   \n",
       "4                                       journal article   \n",
       "...                                                 ...   \n",
       "4995  journal article,multicenter study,observationa...   \n",
       "4996        journal article,randomized controlled trial   \n",
       "4997                                    journal article   \n",
       "4998                                    journal article   \n",
       "4999                                    journal article   \n",
       "\n",
       "                                mesh_keyword_identified  \\\n",
       "0     ciprofloxacin,cipro,staphylococcus,vancomycin,...   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "4995                                                NaN   \n",
       "4996                                                NaN   \n",
       "4997                                                NaN   \n",
       "4998                                                NaN   \n",
       "4999                                                NaN   \n",
       "\n",
       "                            organization_type_1st_level  \\\n",
       "0                                                   NaN   \n",
       "1           university,university,university,university   \n",
       "2     university,university,university,university,un...   \n",
       "3     university,university,university,university,un...   \n",
       "4                                  center,center,center   \n",
       "...                                                 ...   \n",
       "4995  university,university,university,hospital,hosp...   \n",
       "4996  university,university,university,university,un...   \n",
       "4997  university,university,university,university,un...   \n",
       "4998                              university,university   \n",
       "4999  university,university,university,university,un...   \n",
       "\n",
       "                               mesh_term_qualified_desc  ...  \\\n",
       "0                                                   NaN  ...   \n",
       "1     diabetes mellitus\\, type 2,glucagon-like pepti...  ...   \n",
       "2     alzheimer disease,coumaric acids,histone deace...  ...   \n",
       "3     adrenergic beta-antagonists,autoantigens,bioma...  ...   \n",
       "4                                              chickens  ...   \n",
       "...                                                 ...  ...   \n",
       "4995  greece,malnutrition,mass screening,nutritional...  ...   \n",
       "4996  adaptation\\, physiological,athletic performanc...  ...   \n",
       "4997  arecaceae,berberine,berberis,coffea,diet\\, hig...  ...   \n",
       "4998                                                NaN  ...   \n",
       "4999  annona,averrhoa,eugenia,fruit,garcinia,plant e...  ...   \n",
       "\n",
       "                                      hospital_copy_txt  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "4995  \"p. and a. kyriakou\" children's hospital,\"p. a...   \n",
       "4996                                                NaN   \n",
       "4997                                                NaN   \n",
       "4998                                                NaN   \n",
       "4999                                                NaN   \n",
       "\n",
       "                             college_copy_txt  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2          south carolina college of pharmacy   \n",
       "3                                         NaN   \n",
       "4                                         NaN   \n",
       "...                                       ...   \n",
       "4995                                      NaN   \n",
       "4996  jagiellonian university medical college   \n",
       "4997                                      NaN   \n",
       "4998                                      NaN   \n",
       "4999                                      NaN   \n",
       "\n",
       "                                          Journal_Title  \\\n",
       "0                       Foodborne Pathogens and Disease   \n",
       "1                        Journal of Medicinal Chemistry   \n",
       "2                        Journal of Medicinal Chemistry   \n",
       "3     American Journal of Physiology - Heart and Cir...   \n",
       "4                 Tropical Animal Health and Production   \n",
       "...                                                 ...   \n",
       "4995                                          Nutrients   \n",
       "4996                                          Nutrients   \n",
       "4997                                          Nutrients   \n",
       "4998                                          Nutrients   \n",
       "4999                                          Nutrients   \n",
       "\n",
       "                                 Journal_Title_copy_txt  \\\n",
       "0                       Foodborne Pathogens and Disease   \n",
       "1                        Journal of Medicinal Chemistry   \n",
       "2                        Journal of Medicinal Chemistry   \n",
       "3     American Journal of Physiology - Heart and Cir...   \n",
       "4                 Tropical Animal Health and Production   \n",
       "...                                                 ...   \n",
       "4995                                          Nutrients   \n",
       "4996                                          Nutrients   \n",
       "4997                                          Nutrients   \n",
       "4998                                          Nutrients   \n",
       "4999                                          Nutrients   \n",
       "\n",
       "                   organization_name_1st_level_copy_txt  \\\n",
       "0                                                   NaN   \n",
       "1     sungkyunkwan university,sungkyunkwan universit...   \n",
       "2     julius maximilian university of w√ºrzburg,medic...   \n",
       "3     university of miami leonard m. miller school o...   \n",
       "4       nagaland centre,nagaland centre,nagaland centre   \n",
       "...                                                 ...   \n",
       "4995  harokopio university,harokopio university,haro...   \n",
       "4996  gda≈Ñsk university of physical education and sp...   \n",
       "4997  university of naples ''federico ii'',universit...   \n",
       "4998      universit√© de rennes 1,universit√© de fribourg   \n",
       "4999  federal university of grande dourados,federal ...   \n",
       "\n",
       "                                  country_name_copy_txt  \\\n",
       "0     republic of,republic of,republic of,republic o...   \n",
       "1                                                   NaN   \n",
       "2     germany,united states,germany,italy,germany,ge...   \n",
       "3                                                   NaN   \n",
       "4                               india,india,india,india   \n",
       "...                                                 ...   \n",
       "4995  greece,greece,greece,greece,greece,greece,gree...   \n",
       "4996  poland,poland,poland,poland,poland,poland,pola...   \n",
       "4997  italy,italy,italy,italy,italy,italy,italy,ital...   \n",
       "4998                                 france,switzerland   \n",
       "4999                 brazil,brazil,brazil,brazil,brazil   \n",
       "\n",
       "                     mesh_term_descriptor_name_copy_txt  \\\n",
       "0                                                   NaN   \n",
       "1     animals,diabetes mellitus\\, type 2,glucagon-li...   \n",
       "2     alzheimer disease,animals,catalytic domain,cel...   \n",
       "3     adrenergic beta-antagonists,animals,autoantige...   \n",
       "4     animals,chickens,female,india,male,ovum,tropic...   \n",
       "...                                                 ...   \n",
       "4995  child,child\\, preschool,cross-sectional studie...   \n",
       "4996  adaptation\\, physiological,adult,athletic perf...   \n",
       "4997  animals,arecaceae,berberine,berberis,coffea,di...   \n",
       "4998  adult,attitude to health,female,humans,male,nu...   \n",
       "4999  animals,annona,averrhoa,brazil,eugenia,fruit,g...   \n",
       "\n",
       "                                               category        PMID  \\\n",
       "0     Chemicals and Drugs,Organisms,Phenomena and Pr...  33769832.0   \n",
       "1     Organisms,Diseases,Chemicals and Drugs,Phenome...  33769827.0   \n",
       "2     Diseases,Psychiatry and Psychology,Organisms,P...  33769811.0   \n",
       "3     Chemicals and Drugs,Organisms,Analytical\\, Dia...  33769915.0   \n",
       "4     Organisms,Anatomy,Phenomena and Processes,Heal...  33770287.0   \n",
       "...                                                 ...         ...   \n",
       "4995  Analytical\\, Diagnostic and Therapeutic Techni...  33924630.0   \n",
       "4996  Phenomena and Processes,Anthropology\\, Educati...  33924645.0   \n",
       "4997  Organisms,Chemicals and Drugs,Phenomena and Pr...  33924725.0   \n",
       "4998  Psychiatry and Psychology,Health Care,Organism...  33924784.0   \n",
       "4999  Organisms,Analytical\\, Diagnostic and Therapeu...  33924791.0   \n",
       "\n",
       "                                          tags_copy_txt  \n",
       "0     2010-2018,isolates methicillin,chicken carcass...  \n",
       "1     free fatty acid,pancreatic cell,glucagon,fatty...  \n",
       "2     melatonin ferulic acid,dpph radical scavenging...  \n",
       "3     col4a3-/- alport mouse,urine albumin,carvedilo...  \n",
       "4     north eastern hill,chicken,yolk,srinidhi,germp...  \n",
       "...                                                 ...  \n",
       "4995  pyms,clinical setting study,dietitian,whogc,su...  \n",
       "4996  leptin oncostatin,vitamin vegetable,interleuki...  \n",
       "4997  aminotransferase hepatic,microbiota hepatic,be...  \n",
       "4998  coerciveness,coercive measure,social norm,nutr...  \n",
       "4999  creatinine urea,solid sugar,pulp,glutamic pyru...  \n",
       "\n",
       "[5000 rows x 88 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat_data=pd.read_excel('../data/recent_5k.xlsx')\n",
    "# pat_data=pd.read_excel('../data/Keyword_validation.xlsx')\n",
    "pat_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combining Title and preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove special characters \n",
    "def remove_special(x):\n",
    "    # new line at sentence end for training purposes\n",
    "    x = x.replace('.', '\\n')\n",
    "    return re.sub('[^A-Za-z \\n]+', ' ', x).lower()\n",
    "    \n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)\n",
    "def remove_stopwords(x):\n",
    "    return ' '.join([word for word in x.split() if word not in stopwords_dict])\n",
    "\n",
    "abstract = pat_data['Abstract'][~pat_data['Abstract'].isna()].apply(remove_stopwords).apply(remove_special).values\n",
    "title = pat_data['Title'][~pat_data['Title'].isna()].apply(remove_stopwords).apply(remove_special).values\n",
    "corpus = np.concatenate([title,abstract], axis=0)\n",
    "data = '\\n'.join(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### patbase cleaning\n",
    "def text_cleaning(text):\n",
    "    \n",
    "    cleaned_txt = re.sub('<[^<]+>', ' ', str(text))\n",
    "    cleaned_txt = re.sub( r'\\[.*?\\]', ' ',cleaned_txt)  #remove brackets\n",
    "    cleaned_txt = re.sub('\\d+', ' ', cleaned_txt)\n",
    "    cleaned_txt = re.sub('\\(\\w+\\)', ' ', cleaned_txt) \n",
    "  \n",
    "#     cleaned_txt=cleaned_txt.translate(str.maketrans(' ', ' ',string.punctuation)) \n",
    "    return cleaned_txt\n",
    "   \n",
    "def stopword_removal(text):\n",
    "    cleaned_txt = ' '.join([token for token in word_tokenize(text) if token not in stop_words])\n",
    "    return cleaned_txt\n",
    "def lemmatize_text(text):\n",
    "    cleaned_txt=' '.join([lemmatizer.lemmatize(token) for token in word_tokenize(text)])\n",
    "    return cleaned_txt\n",
    "\n",
    "def noun_adj_fetcher(cleaned_txt):\n",
    "    Noun_adj=[w for w , pos in pos_tag(str(cleaned_txt).split()) if (pos == 'NN' or pos=='JJ' or pos=='JJR' or pos=='JJS' )]\n",
    "    return Noun_adj\n",
    "\n",
    "# python -m spacy download en_core_web_sm\n",
    "def noun_adj_pairs(text):\n",
    "    doc = nlp(str(text))\n",
    "    noun_adj_pairs = []\n",
    "    for i,token in enumerate(doc):\n",
    "        if token.pos_ not in ('NOUN','PROPN'):\n",
    "            continue\n",
    "        for j in range(i+1,len(doc)):\n",
    "            if doc[j].pos_ == 'ADJ':\n",
    "                noun_adj_pairs.append((token,doc[j]))\n",
    "                break\n",
    "    return noun_adj_pairs\n",
    "\n",
    "def preprocessed_text(df):\n",
    "    \n",
    "#     col=['nltk_tokens','nouns','lemmatize_text']\n",
    "    preprocess_txt=pd.DataFrame()\n",
    "    abs_clean_text=df.apply(text_cleaning)\n",
    "    preprocess_txt['lemmatize_text']=abs_clean_text.apply(stopword_removal)         \n",
    "#     preprocess_txt['lemmatize_text']=stopword_removed_text.apply(lemmatize_text)\n",
    "  \n",
    "#     for ind,text in preprocess_txt['lemmatize_text'].iteritems():       \n",
    "#         tokens=word_tokenize(text)\n",
    "#         pos_tokens=nltk.pos_tag(tokens)\n",
    "# #         preprocess_txt.at[ind,'nouns']=[pos_token for pos_token,pos in pos_tokens if pos.startswith('N')]\n",
    "# #         preprocess_txt.at[ind,'nltk_tokens']=tokens     \n",
    "    return preprocess_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>contamination meat antimicrobial-resistant bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g-protein-coupled receptor ( gpr ) considered ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>structures melatonin ferulic acid merged terti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>currently food drug administration-approved tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>productivity traditional backyard poultry deve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>nutritional risk screening yet established man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>purpose : growing number studies indicate impo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>non-alcoholic-fatty liver disease spreading wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>propose test model food policy acceptability ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>study aimed analyze physicochemical characteri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         lemmatize_text\n",
       "0     contamination meat antimicrobial-resistant bac...\n",
       "1     g-protein-coupled receptor ( gpr ) considered ...\n",
       "2     structures melatonin ferulic acid merged terti...\n",
       "3     currently food drug administration-approved tr...\n",
       "4     productivity traditional backyard poultry deve...\n",
       "...                                                 ...\n",
       "4995  nutritional risk screening yet established man...\n",
       "4996  purpose : growing number studies indicate impo...\n",
       "4997  non-alcoholic-fatty liver disease spreading wo...\n",
       "4998  propose test model food policy acceptability ....\n",
       "4999  study aimed analyze physicochemical characteri...\n",
       "\n",
       "[5000 rows x 1 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_text=pat_data['Abstract']\n",
    "\n",
    "# abs_text=pat_data['Title']\n",
    "# abs_text[1]\n",
    "\n",
    "preprocessed_df1=preprocessed_text(abs_text)\n",
    "preprocessed_df1\n",
    "# preprocessed_df1['Noun_adj']=abs_text.apply(noun_adj_pairs)\n",
    "# corpus='\\n'.join([i for i in preprocessed_df1['lemmatize_text']])\n",
    "# print((corpus[:100]))\n",
    "# corpus=' '.join(preprocessed_df1['lemmatize_text'])\n",
    "# corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contamination meat antimicrobial-resistant bacteria represents major public health threat worldwide . study\\ , determined antimicrobial resistance profiles resistance trends staphylococcus aureus isolated major food animal carcasses ( cattle\\ , pig\\ , chicken carcass isolates ) korea . approximately % \\ , % \\ , % cattle\\ , pig\\ , chicken carcass isolates\\ , respectively\\ , resistant least one antimicrobial agent . resistance penicillin ( . % ) highest\\ , followed resistance tetracycline ( . % ) erythromycin ( . % ) . % pig chicken isolates resistant ciprofloxacin . observed linezolid resistance pig isolates ( . % ) . however\\ , s. aureus isolates sensitive rifampin vancomycin . noted increasing fluctuating trend kanamycin penicillin resistance cattle isolates . similarly\\ , chloramphenicol\\ , ciprofloxacin\\ , tetracycline\\ , trimethoprim resistance rates increased fluctuated time pig isolates . methicillin-resistant s. aureus accounted % \\ , % \\ , % cattle\\ , pig\\ , chicken isolates\\ , respectively . mrsa strains exhibited significantly high resistance rates tested antimicrobials\\ , including ciprofloxacin\\ , erythromycin\\ , tetracycline compared methicillin-susceptible s. aureus strains . notably\\ , relatively high percentage mrsa strains ( . % ) recovered pig carcasses resistant linezolid compared mssa strains ( . % ) . addition\\ , almost % isolates multi-drug resistant . s. aureus isolates recovered major food animal carcasses korea exhibited resistance clinically important antimicrobials\\ , posing public health risk .\n"
     ]
    }
   ],
   "source": [
    "sample_pat_data=preprocessed_df1['lemmatize_text'][0]\n",
    "sample_pat_data=text_cleaning(sample_pat_data).lower()\n",
    "print(sample_pat_data)\n",
    "\n",
    "\n",
    "class Sentence(NamedTuple):\n",
    "    \"\"\"\n",
    "    a data model for a sentence implemented using NamedTuple\n",
    "    \"\"\"\n",
    "    words: List[str]\n",
    "    pos: List[str]\n",
    "    lemmas: List[str]\n",
    "    length: int\n",
    "\n",
    "\n",
    "class Document(NamedTuple):\n",
    "    \"\"\"\n",
    "    a data model for document implemented using NamedTuple\n",
    "    \"\"\"\n",
    "    sentences: List[Sentence]\n",
    "\n",
    "def process(text, max_length=None):\n",
    "    \"\"\"\n",
    "        given a text, call spacy for sentence splitting, lemmatisation, and POS tagging\n",
    "        :param text:\n",
    "        :param max_length:\n",
    "        :return: a document with processed sentences\n",
    "    \"\"\"\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    sentences = []\n",
    "    for sent in doc.sents:\n",
    "        sentences.append({\n",
    "                \"words\": [token.text for token in sent],\n",
    "                \"lemmas\": [token.lemma_ for token in sent],\n",
    "                \"pos\": [token.pos_ for token in sent],\n",
    "        })\n",
    "\n",
    "    return Document(sentences=[Sentence(words=s['words'], pos=s['pos'], lemmas=s['lemmas'], length=len(s['words']))\n",
    "                                   for s in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_graph(window=None, pos=None):\n",
    "    graph = nx.Graph()\n",
    "    \n",
    "    # use lemmas to build a word graph\n",
    "    text = [(word, sentence.pos[i] in pos) for sentence in sentences for i, word in enumerate(sentence.lemmas)]\n",
    "\n",
    "    # add nodes\n",
    "    graph.add_nodes_from([word for word, valid in text if valid])\n",
    "\n",
    "    # add edges\n",
    "    for i, (node1, is_valid1) in enumerate(text):\n",
    "        # skip if a word is neither a noun nor an adjective\n",
    "        if not is_valid1:\n",
    "            continue\n",
    "\n",
    "        # add an edge with node2 within a window size\n",
    "        for j in range(i + 1, min(i + window, len(text))):\n",
    "            node2, is_valid2 = text[j]\n",
    "            if is_valid2 and node1 != node2:\n",
    "                graph.add_edge(node1, node2)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x1c2b2b1ad00>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=process(sample_pat_data,max_length=10**5)\n",
    "sentences=doc.sentences\n",
    "VALID_POSTAGS = {'ADJ','NOUN'}\n",
    "graph=build_word_graph(window=3,pos=VALID_POSTAGS)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_longest_keyword_sequences(keywords):\n",
    "    \"\"\"\n",
    "    Select longest sequences of keywords\n",
    "    :param keywords: a list of keywords used for building a word graph\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # a list of valid token offsets\n",
    "        sequence = []\n",
    "        candi=[]\n",
    "        for j, token in enumerate(sentences[i].lemmas):\n",
    "            # add an offset of a valid token into longest_sequence\n",
    "            if token in keywords:\n",
    "                sequence.append(j)\n",
    "                if j < (sentence.length - 1):\n",
    "                    continue\n",
    "            # when a token is not valid check if there is a sequence\n",
    "            if sequence:\n",
    "                start = sequence[0]\n",
    "                end = sequence[-1] + 1\n",
    "             \n",
    "                cand=add_candidate(words=sentence.words[start:end],\n",
    "                                       lemmas=sentence.lemmas[start:end],\n",
    "                                       pos=sentence.pos[start:end],\n",
    "                                       sentence_id=i)\n",
    "                \n",
    "                candi.append(cand)\n",
    "            sequence=[]\n",
    "        return candi\n",
    "# # reset the list\n",
    "#     sequence = []   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Candidate(object):\n",
    "    \"\"\"\n",
    "    data model for keyword/phrase candidates\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        variable initialization\n",
    "        \"\"\"\n",
    "        self.lexical_form = []\n",
    "        self.surface_forms = []\n",
    "        self.sentence_ids = []\n",
    "        self.pos_patterns = []\n",
    "    \n",
    "    def add_candidate(words, lemmas, pos, sentence_id):\n",
    "        \"\"\"\n",
    "            populate candidates for a document\n",
    "            :param words:\n",
    "            :param lemmas:\n",
    "            :param pos:\n",
    "            :param sentence_id:\n",
    "            :return:\n",
    "        \"\"\"\n",
    "        candidates = defaultdict(Candidate)\n",
    "        key = ' '.join(words).lower()\n",
    "\n",
    "        candidates[key].lexical_form = lemmas\n",
    "        candidates[key].surface_forms.append(words)\n",
    "        candidates[key].pos_patterns.append(pos)\n",
    "        candidates[key].sentence_ids.append(sentence_id)\n",
    "        return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(normalized=False):\n",
    "        \"\"\"\n",
    "        Rank the longest sequences\n",
    "        :param normalized:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        weights = {}\n",
    "        # calculate PageRank score for each node using default values\n",
    "        w = nx.pagerank_numpy(graph, alpha=0.5, weight=None)\n",
    "#         print(w)\n",
    "        wt=0\n",
    "        keywords = ','.join(sorted(w, key=w.get, reverse=True))\n",
    "#         print('keywords',keywords)\n",
    "        \n",
    "        cand=select_longest_keyword_sequences(keywords)\n",
    "#         print('cand:',cand)\n",
    "        tokens= [j for i in cand for j in i.keys()]\n",
    "#         print('tokens',tokens)\n",
    "        for i in tokens:\n",
    "            for j in i.split():\n",
    "                if j in w.keys():\n",
    "                    wt+=w[j]\n",
    "                    weights[i]=wt\n",
    "                    if normalized:\n",
    "                        weights[i] /= len(tokens)            \n",
    "        return weights,cand,keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-1ed89d36944c>:9: DeprecationWarning: networkx.pagerank_numpy is deprecated and will be removed in NetworkX 3.0, use networkx.pagerank instead.\n",
      "  w = nx.pagerank_numpy(graph, alpha=0.5, weight=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'contamination meat antimicrobial - resistant bacteria': 0.0673646975523209,\n",
       " 'major public health threat': 0.11124821143325522}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights,cand,keywords=rank_candidates(normalized=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-55-1ed89d36944c>:9: DeprecationWarning: networkx.pagerank_numpy is deprecated and will be removed in NetworkX 3.0, use networkx.pagerank instead.\n",
      "  w = nx.pagerank_numpy(graph, alpha=0.5, weight=None)\n",
      "5it [00:03,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "col=['Abstract','Keyword_extracted','Keywords_ranked']\n",
    "keyword_tagging_df=pd.DataFrame(columns=col)\n",
    "keyword_tagging_df['Abstract']=pat_data['Abstract']\n",
    "keyword_tagging_df['Keyword_extracted']=pat_data['keyword_copy_txt']\n",
    "\n",
    "\n",
    "    \n",
    "for index,text in tqdm(enumerate(preprocessed_df1['lemmatize_text'][:5])):        \n",
    "    doc=process(text,max_length=10**5)\n",
    "    sentences=doc.sentences\n",
    "    VALID_POSTAGS = {'NOUN'}\n",
    "    graph=build_word_graph(window=3,pos=VALID_POSTAGS)\n",
    "    weights,cand,keywords=rank_candidates(normalized=False)\n",
    "    keyword_tagging_df.loc[index].Keywords_ranked=keywords\n",
    "    \n",
    "#     cons_keys=consolidate_duplicate_candidates(threshold=0.9)\n",
    "#     print('consolidated keys',cons_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Keyword_extracted</th>\n",
       "      <th>Keywords_ranked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>contamination of meat with antimicrobial-resis...</td>\n",
       "      <td>s. aureus,antimicrobial resistance,carcasses,f...</td>\n",
       "      <td>resistance,trend,isolate,carcass,health,pig,%,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g-protein-coupled receptor 40 (gpr40) is consi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gpr,acid,secretion,Œ≤,type,gsis,target,agonist,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the structures of melatonin and ferulic acid w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hdac,melatonin,ability,inhibitor,dysfunction,s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there are currently no food and drug administr...</td>\n",
       "      <td>hfpef,carvedilol,diastolic dysfunction,exercis...</td>\n",
       "      <td>carvedilol,alport,treatment,function,col,model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>productivity of traditional backyard poultry i...</td>\n",
       "      <td>backyard poultry production,north eastern hill...</td>\n",
       "      <td>parent,vanaraja,week,mortality,egg,backyard,ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>nutritional risk screening (nrs) is not yet es...</td>\n",
       "      <td>pyms,stamp,who,malnutrition,nutritional risk,p...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>purpose: a growing number of studies indicate ...</td>\n",
       "      <td>il-6,inflammation,resistin,skeletal muscle dam...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>non-alcoholic-fatty liver disease (nafld) is s...</td>\n",
       "      <td>nafld,gut microbiome,insulin resistance,metabo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>we propose and test a model of food policy acc...</td>\n",
       "      <td>acceptability,food policy,survey</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>this study aimed to analyze the physicochemica...</td>\n",
       "      <td>amazonian fruits,composition,metabolic effects</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Abstract  \\\n",
       "0     contamination of meat with antimicrobial-resis...   \n",
       "1     g-protein-coupled receptor 40 (gpr40) is consi...   \n",
       "2     the structures of melatonin and ferulic acid w...   \n",
       "3     there are currently no food and drug administr...   \n",
       "4     productivity of traditional backyard poultry i...   \n",
       "...                                                 ...   \n",
       "4995  nutritional risk screening (nrs) is not yet es...   \n",
       "4996  purpose: a growing number of studies indicate ...   \n",
       "4997  non-alcoholic-fatty liver disease (nafld) is s...   \n",
       "4998  we propose and test a model of food policy acc...   \n",
       "4999  this study aimed to analyze the physicochemica...   \n",
       "\n",
       "                                      Keyword_extracted  \\\n",
       "0     s. aureus,antimicrobial resistance,carcasses,f...   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3     hfpef,carvedilol,diastolic dysfunction,exercis...   \n",
       "4     backyard poultry production,north eastern hill...   \n",
       "...                                                 ...   \n",
       "4995  pyms,stamp,who,malnutrition,nutritional risk,p...   \n",
       "4996  il-6,inflammation,resistin,skeletal muscle dam...   \n",
       "4997  nafld,gut microbiome,insulin resistance,metabo...   \n",
       "4998                   acceptability,food policy,survey   \n",
       "4999     amazonian fruits,composition,metabolic effects   \n",
       "\n",
       "                                        Keywords_ranked  \n",
       "0     resistance,trend,isolate,carcass,health,pig,%,...  \n",
       "1     gpr,acid,secretion,Œ≤,type,gsis,target,agonist,...  \n",
       "2     hdac,melatonin,ability,inhibitor,dysfunction,s...  \n",
       "3     carvedilol,alport,treatment,function,col,model...  \n",
       "4     parent,vanaraja,week,mortality,egg,backyard,ca...  \n",
       "...                                                 ...  \n",
       "4995                                                NaN  \n",
       "4996                                                NaN  \n",
       "4997                                                NaN  \n",
       "4998                                                NaN  \n",
       "4999                                                NaN  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keyword_tagging_df.to_csv('V:/ML_projects/Merckgroup/Keyword_tagging/preprocess_output/.csv')\n",
    "keyword_tagging_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_duplicate_candidates(threshold=None):\n",
    "    \"\"\"\n",
    "    Filter the candidates\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    weights,candidates,keyword=rank_candidates(normalized=False)\n",
    "    \n",
    "    candidates_list=[j for i in candidates for j in i.keys()]\n",
    "    \n",
    "    top_keys = sorted(weights, key=weights.get, reverse=True)\n",
    "#     print('''\\n \\ntop keys\\n''',top_keys)\n",
    "    w = nx.pagerank_numpy(graph, alpha=0.5, weight=None)\n",
    "#     print(len([i.keys() for i in candidates]))\n",
    "    \n",
    "    keywords = sorted(w, key=w.get, reverse=True)\n",
    "    candidates=select_longest_keyword_sequences(keywords)\n",
    "\n",
    "    keys_to_consolidate = []\n",
    "    \n",
    "    for (k1, k2) in combinations(top_keys, 2):\n",
    "\n",
    "        if k1 == k2:\n",
    "            continue\n",
    "        # calculate distance between key phrases\n",
    "#         distance = model.wv.wmdistance(k1, k2)\n",
    "# #         print('distance',distance)\n",
    "#         print(k1,'dist',k2,'-->',distance)\n",
    "#         # remove a lower ranked candidate\n",
    "#         if distance > threshold:\n",
    "#             keys_to_consolidate.append((k1,k2))\n",
    "# #     print('keys to consolidate',keys_to_consolidate)\n",
    "\n",
    "#     return(keys_to_consolidate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-1ed89d36944c>:9: DeprecationWarning: networkx.pagerank_numpy is deprecated and will be removed in NetworkX 3.0, use networkx.pagerank instead.\n",
      "  w = nx.pagerank_numpy(graph, alpha=0.5, weight=None)\n",
      "<ipython-input-60-7b6cdd2c7a6b>:12: DeprecationWarning: networkx.pagerank_numpy is deprecated and will be removed in NetworkX 3.0, use networkx.pagerank instead.\n",
      "  w = nx.pagerank_numpy(graph, alpha=0.5, weight=None)\n"
     ]
    }
   ],
   "source": [
    "consolidate_duplicate_candidates(threshold=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n=15\n",
    "diversity=0.3\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr(doc_embedding: np.ndarray,word_embeddings: np.ndarray,words,top_n,diversity):\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "    \n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "    \n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "#         mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        \n",
    "        if mmr.size>0:\n",
    "            mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "            \n",
    "            # Update keywords & candidates\n",
    "            keywords_idx.append(mmr_idx)\n",
    "            candidates_idx.remove(mmr_idx)\n",
    "            \n",
    "    return [(words[idx], round((word_doc_similarity.reshape(1, -1)[0][idx]), 4)) for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_word_doc_embedding(doc):\n",
    "    cv_countvec = CountVectorizer(ngram_range=(1,1), stop_words=stop_words, min_df=1).fit([doc])\n",
    "    cv_words = cv_countvec.get_feature_names()\n",
    "    cv_doc_embeddings = embedding_model.encode([doc])\n",
    "    cv_word_embeddings =embedding_model.encode(cv_words)\n",
    "    return cv_word_embeddings,cv_doc_embeddings,cv_words\n",
    "    \n",
    "def tfidf_word_doc_embedding(doc):\n",
    "    tfidf_countvec = TfidfVectorizer(ngram_range=(1,1), stop_words=stop_words, min_df=0.8).fit([doc])\n",
    "    tfidf_words = tfidf_countvec.get_feature_names()    \n",
    "\n",
    "    tfidf_doc_embeddings = embedding_model.encode([doc])\n",
    "    tfidf_word_embeddings = embedding_model.encode(tfidf_words)\n",
    "    return tfidf_word_embeddings,tfidf_doc_embeddings,tfidf_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    cleaned_txt = re.sub('<[^<]+>', ' ', str(text))\n",
    "    cleaned_txt = re.sub( r'\\[.*?\\]', ' ',cleaned_txt)  #remove brackets\n",
    "#     cleaned_txt = re.sub('\\d+', ' ', cleaned_txt)\n",
    "#     cleaned_txt = re.sub('\\(\\w+\\)', ' ', cleaned_txt)\n",
    "    cleaned_txt=text.translate(str.maketrans(' ', ' ',string.punctuation)) \n",
    "    return cleaned_txt\n",
    "   \n",
    "def stopword_removal(text):\n",
    "    cleaned_txt = ' '.join([token for token in word_tokenize(text) if token not in stop_words])\n",
    "    return cleaned_txt\n",
    "def lemmatize_text(text):\n",
    "    cleaned_txt=' '.join([lemmatizer.lemmatize(token) for token in word_tokenize(text)])\n",
    "\n",
    "    return cleaned_txt\n",
    "\n",
    "def preprocessed_text(df):\n",
    "    \n",
    "    col=['nltk_tokens','nouns_verbs','lemmatize_text']\n",
    "    preprocess_txt=pd.DataFrame(columns=col)\n",
    "    abs_clean_text=df.apply(text_cleaning)\n",
    "    \n",
    "    stopword_removed_text=abs_clean_text.apply(stopword_removal)         \n",
    "    preprocess_txt['lemmatize_text']=stopword_removed_text.apply(lemmatize_text)\n",
    "  \n",
    "    for ind,text in preprocess_txt['lemmatize_text'].iteritems():       \n",
    "        tokens=word_tokenize(text)\n",
    "        pos_tokens=nltk.pos_tag(tokens)\n",
    "        preprocess_txt.at[ind,'nouns_verbs']=[pos_token for pos_token,pos in pos_tokens if pos.startswith('N' or 'V')]\n",
    "        preprocess_txt.at[ind,'nltk_tokens']=tokens     \n",
    "    return preprocess_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 14.36it/s]\n"
     ]
    }
   ],
   "source": [
    "doc=preprocessed_df1['lemmatize_text'][:100]\n",
    "\n",
    "tfidf_keywords=[]\n",
    "tfidf_phrases=[]\n",
    "for i in tqdm(doc):\n",
    "    tfidf_word_embed,tfidf_doc_embed,tfidf_words=tfidf_word_doc_embedding(i)\n",
    "    tfidf_keywords_doc=mmr(tfidf_doc_embed,tfidf_word_embed,tfidf_words,5,0.2)\n",
    "    tfidf_keywords_doc=','.join([key[0] for key in tfidf_keywords_doc])\n",
    "    \n",
    "    tfidf_keywords.append(tfidf_keywords_doc)\n",
    "#     tfidf_phrases.append(tfidf_phrases_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['antimicrobials,pig,resistance,isolates,bacteria',\n",
       " 'insulin,gpr,receptor,glycemic,diabetes',\n",
       " 'inhibitors,melatonin,hdac,antioxidant,nm',\n",
       " 'dysfunction,cardiac,adrenergic,drug,renal',\n",
       " 'poultry,chicken,srinidhi,backyard,hen',\n",
       " 'staining,brains,samples,cadaver,clearing',\n",
       " 'angiotensin,immunosuppression,fibroblasts,antitumor,macrophages',\n",
       " 'crystal,prediction,framework,organic,molecule',\n",
       " 'hydrogels,gels,biomacromolecules,tissues,gel',\n",
       " 'food,simulation,powders,heat,compositional',\n",
       " 'pore,adsorption,porosimetry,nanoporous,physisorption',\n",
       " 'stigma,dietary,severity,levels,health',\n",
       " 'mutations,mpl,myelofibrosis,exon,mutation',\n",
       " 'sport,risks,statistical,association,reviewed',\n",
       " 'neuropathy,elbow,myositis,nerves,pain']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_keywords[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 25.19it/s]\n"
     ]
    }
   ],
   "source": [
    "cv_keywords=[]\n",
    "cv_phrases=[]\n",
    "for i in tqdm(doc):\n",
    "    cv_word_embed,cv_doc_embed,cv_words=cv_word_doc_embedding(i)\n",
    "    cv_keywords_doc=mmr(cv_doc_embed,cv_word_embed,cv_words,10,0.2)\n",
    "    cv_keywords_doc=','.join([key[0] for key in cv_keywords_doc])\n",
    "    \n",
    "    cv_keywords.append(cv_keywords_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['antimicrobials,pig,resistance,isolates,bacteria,aureus,carcass,risk,staphylococcus,contamination',\n",
       " 'insulin,gpr,receptor,glycemic,diabetes,peptide,glp,drug,gsis,glucose',\n",
       " 'inhibitors,melatonin,hdac,antioxidant,nm,aŒ≤,deacetylase,ic,acid,neurotoxicity',\n",
       " 'dysfunction,cardiac,adrenergic,drug,renal,hfpef,cardiorenal,levels,activity,mice',\n",
       " 'poultry,chicken,srinidhi,backyard,hen,reproductive,growth,climatic,vanaraja,birds',\n",
       " 'staining,brains,samples,cadaver,clearing,antibody,optimized,microscopy,evaluating,quality',\n",
       " 'angiotensin,immunosuppression,fibroblasts,antitumor,macrophages,receptor,therapeutic,tumor,immunity,chemokine',\n",
       " 'crystal,prediction,framework,organic,molecule,research,pharmaceutical,methodologies,agrochemical,theoretical',\n",
       " 'hydrogels,gels,biomacromolecules,tissues,gel,toughening,tendon,bonds,biomedical,ligament',\n",
       " 'food,simulation,powders,heat,compositional,agglomeration,industry,mass,flowability,model',\n",
       " 'pore,adsorption,porosimetry,nanoporous,physisorption,porous,protocols,molecular,pores,gas',\n",
       " 'stigma,dietary,severity,levels,health,iws,weight,populations,insecurity,concerns',\n",
       " 'mutations,mpl,myelofibrosis,exon,mutation,myeloproliferative,diagnosis,sequencing,thrombocythemia,neoplasms',\n",
       " 'sport,risks,statistical,association,reviewed,injury,factor,sportdiscus,screening,asymmetry',\n",
       " 'neuropathy,elbow,myositis,nerves,pain,finger,peripheral,nerve,forearm,complained']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf_keywords\n",
    "# preprocessed_df1['tfidf_mmr']=tfidf_keywords\n",
    "cv_keywords[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_generator(docs,words,countvec):\n",
    "    \n",
    "    top_n:int=10\n",
    "    diversity:0.5\n",
    "    df = countvec.transform(docs)\n",
    "    \n",
    "    keywords=[]    \n",
    "    phrases=[]\n",
    "    \n",
    "    \n",
    "    for index, doc in enumerate(docs):\n",
    "        doc_words = [words[i] for i in df[index].nonzero()[1]]\n",
    "        if doc_words:\n",
    "            doc_word_embeddings = np.array([tfidf_word_embeddings[i] for i in df[index].nonzero()[1]])\n",
    "            distances = cosine_similarity([tfidf_doc_embeddings[index]], doc_word_embeddings)[0]\n",
    "            doc_keywords = [(doc_words[i], round(float(distances[i]), 4)) for i in distances.argsort()[-top_n:]]\n",
    "            doc_keywords=','.join([key[0] for key in doc_keywords])\n",
    "            doc_phrases=''.join([key[0] for key in doc_keywords])\n",
    "            keywords.append(doc_keywords)\n",
    "            phrases.append(doc_phrases)                                 \n",
    "    return keywords,phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs1=preprocessed_df1['lemmatize_text']\n",
    "tfidf_countvec = TfidfVectorizer(ngram_range=(1,1), stop_words=stop_words, min_df=1).fit(docs1)\n",
    "tfidf_words = tfidf_countvec.get_feature_names()\n",
    "\n",
    "tfidf_doc_embeddings = embedding_model.encode(docs1)\n",
    "tfidf_word_embeddings = embedding_model.encode(tfidf_words)\n",
    "\n",
    "tfidf_keywords_doc,tfidf_phrases_doc=phrase_generator(docs1,tfidf_words,tfidf_countvec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../word_embedding/bio_embedding_intrinsic.bin'\n",
    "Bioword_intrinsic_model=KeyedVectors.load_word2vec_format(path,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kmapper as km\n",
    "# # Project all of the \"sentences\" down into a 2D representation\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# from sklearn.manifold import Isomap\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn import cluster\n",
    "\n",
    "# def keyphrase_mapper(phrases):\n",
    "#     mapper = km.KeplerMapper(verbose=2)\n",
    "\n",
    "#     projected_X = mapper.fit_transform(np.array(phrases),\n",
    "#     projection=[TfidfVectorizer(analyzer=\"char\",\n",
    "#                             ngram_range=(1,6),\n",
    "#                             max_df=0.85,\n",
    "#                             min_df=0.05),\n",
    "#             TruncatedSVD(n_components=100,\n",
    "#                          random_state=1729),\n",
    "#             Isomap(n_components=2,\n",
    "#                    n_jobs=-1)],\n",
    "#     scaler=[None, None, MinMaxScaler()])\n",
    "\n",
    "#     print(\"SHAPE\",projected_X.shape)\n",
    "\n",
    "#     return projected_X\n",
    "\n",
    "# # Cluster the projected data\n",
    "# def cluster_visualization(docs,projected_X,path):\n",
    "#     mapper = km.KeplerMapper(verbose=2)\n",
    "#     graph = mapper.map(projected_X, clusterer=cluster.DBSCAN(eps=0.5, min_samples=3))\n",
    "#     # Get counts of phrases from data (to be used for the label and for cluster statistics)\n",
    "#     vec = TfidfVectorizer(analyzer=\"word\",\n",
    "#                   strip_accents=\"unicode\",\n",
    "#                   stop_words=\"english\",\n",
    "#                   ngram_range=(1,3),\n",
    "#                   max_df=0.9,\n",
    "#                   min_df=0.02)\n",
    "\n",
    "#     interpretable_inverse_X = vec.fit_transform(docs).toarray()\n",
    "#     interpretable_inverse_X_names = vec.get_feature_names()\n",
    "\n",
    "#     print(\"SHAPE\", interpretable_inverse_X.shape)\n",
    "#     print(\"FEATURE NAMES SAMPLE\",\n",
    "#     interpretable_inverse_X_names[:400])\n",
    "#     html = mapper.visualize(graph,X=interpretable_inverse_X,\n",
    "#                         X_names=interpretable_inverse_X_names,\n",
    "#                         path_html=path,\n",
    "#                         lens=projected_X,\n",
    "#                         lens_names=[\"ISOMAP1\", \"ISOMAP2\"],\n",
    "#                         title=\"Visualizing Text - Abstract\",\n",
    "#                         )\n",
    "\n",
    "# projected_tfidf_X=keyphrase_mapper(tfidf_phrases_doc)\n",
    "\n",
    "# cluster_visualization(docs1,projected_tfidf_X,'Abstract_tfidf_mmr_final.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_duplicate_candidates(threshold=None):\n",
    "    \"\"\"\n",
    "    Filter the candidates\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    weights,candidates,keyword=rank_candidates(normalized=False)\n",
    "    \n",
    "    candidates_list=[j for i in candidates for j in i.keys()]\n",
    "    \n",
    "    top_keys = sorted(weights, key=weights.get, reverse=True)\n",
    "    print('''\\n \\ntop keys\\n''',top_keys)\n",
    "    w = nx.pagerank_numpy(graph, alpha=0.5, weight=None)\n",
    "    print(len([i.keys() for i in candidates]))\n",
    "    \n",
    "    keywords = sorted(w, key=w.get, reverse=True)\n",
    "    candidates=select_longest_keyword_sequences(keywords)\n",
    "\n",
    "    keys_to_consolidate = []\n",
    "    reasons_to_consolidate =[]\n",
    "    \n",
    "    for (k1, k2) in combinations(top_keys, 2):\n",
    "\n",
    "        if k1 == k2:\n",
    "            continue\n",
    "        # calculate distance between key phrases\n",
    "#         distance =Bioword_intrinsic_model.wmdistance(k1,k2)\n",
    "        distance =Bioword_intrinsic_model.wmdistance(k1,k2)\n",
    "        print('distance',distance)\n",
    "        print(k1,'dist',k2,'-->',distance)\n",
    "        # remove a lower ranked candidate\n",
    "        if distance > threshold:\n",
    "            keys_to_consolidate.append(k2)\n",
    "            reasons_to_consolidate.append((k2, k1))\n",
    "    # delete candidates selected from above\n",
    "#     print(candidates_list)\n",
    "#     keys_to_delete=list(set(keys_to_delete))\n",
    "    print('keys to consolidate',keys_to_consolidate)\n",
    "#     res = list(filter(lambda i: i not in keys_to_delete, candidates_list))\n",
    "#     print('res',res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition dist backyard chicken --> 0.027770979799234596\n",
      "condition dist introduction --> 0.00914646026217833\n",
      "condition dist backyard poultry --> 0.029565351907635918\n",
      "backyard chicken dist introduction --> 0.02635782929161031\n",
      "backyard chicken dist backyard poultry --> 0.017078582547482336\n",
      "introduction dist backyard poultry --> 0.023584520742763463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('condition', 'backyard chicken'),\n",
       " ('condition', 'backyard poultry'),\n",
       " ('backyard chicken', 'introduction'),\n",
       " ('backyard chicken', 'backyard poultry'),\n",
       " ('introduction', 'backyard poultry')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consolidate_duplicate_candidates(threshold=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "from torch import nn\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=64)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=64, activation_function=nn.Tanh())\n",
    "sent_model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\n",
    "# pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "# dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "# sent_model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_embedding_model = models.Transformer('distilbert-base-uncased', max_seq_length=128)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "sent_model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Doc MMR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_word_doc_embedding(doc):\n",
    "    tfidf_countvec = TfidfVectorizer(ngram_range=(1,1), stop_words=stop_words, min_df=0.8).fit([doc])\n",
    "    tfidf_words = tfidf_countvec.get_feature_names()    \n",
    "\n",
    "    tfidf_doc_embeddings = sent_model.encode([doc])\n",
    "    tfidf_word_embeddings = sent_model.encode(tfidf_words)\n",
    "    return tfidf_word_embeddings,tfidf_doc_embeddings,tfidf_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:09<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "doc=preprocessed_df1[:10]\n",
    "tfidf_BERT_keywords=[]\n",
    "for i in tqdm(doc):\n",
    "    tfidf_word_embed,tfidf_doc_embed,tfidf_words=tfidf_word_doc_embedding(i)\n",
    "    tfidf_keywords_doc=mmr_consolidate(tfidf_doc_embed,tfidf_word_embed,tfidf_words,5,0.2)\n",
    "    tfidf_keywords_doc=','.join([key[0] for key in tfidf_keywords_doc])    \n",
    "    tfidf_BERT_keywords.append(tfidf_keywords_doc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['throughput,pooled,crispr,cas9,customizing',\n",
       " 'prokaryotes,plasmids,epidermidis,deletion,antisense',\n",
       " 'throughput,aligner,visualization,multiplexing,nxuning',\n",
       " 'helicase,repressor,transcriptional,nuclease,infect',\n",
       " 'cytometry,plasmid,qpcr,interspaced,palindromic',\n",
       " 'biotechnological,prokaryotic,translational,controllable,palindromic',\n",
       " 'elucidate,epidemiology,epidemiological,antagonize,phage',\n",
       " 'refseq,throughput,oligonucleotides,nucleotide,sgrnas',\n",
       " 'photosensor,epigenome,acriia4,streptococcus,spatiotemporally',\n",
       " 'heuristic,predefined,mismatches,biotechnologies,benchmark']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_BERT_keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_generator(docs,words,countvec):\n",
    "    \n",
    "    top_n:int=15\n",
    "    diversity:0.5\n",
    "    df = countvec.transform(docs)\n",
    "    \n",
    "    keywords=[]    \n",
    "    \n",
    "    \n",
    "    for index, doc in enumerate(tqdm(docs)):\n",
    "        doc_words = [words[i] for i in df[index].nonzero()[1]]\n",
    "        if doc_words:\n",
    "            doc_word_embeddings = np.array([tfidf_word_embeddings_bert[i] for i in df[index].nonzero()[1]])\n",
    "            distances = cosine_similarity([tfidf_doc_embeddings_bert[index]], doc_word_embeddings)[0]\n",
    "            doc_keywords = [(doc_words[i], round(float(distances[i]), 4)) for i in distances.argsort()[-top_n:]]\n",
    "            doc_keywords=[key[0] for key in doc_keywords]\n",
    "            keywords.append(doc_keywords)\n",
    "#             phrases.append(doc_phrases)                                 \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docs1=preprocessed_df1['lemmatize_text'][:10]\n",
    "tfidf_countvec = TfidfVectorizer(ngram_range=(1,1), stop_words=stop_words, min_df=1).fit(docs1)\n",
    "tfidf_words = tfidf_countvec.get_feature_names()\n",
    "\n",
    "tfidf_doc_embeddings_bert = sent_model.encode(docs1)\n",
    "tfidf_word_embeddings_bert = sent_model.encode(tfidf_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 415.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['carcass',\n",
       "  'carcasses',\n",
       "  'fluctuating',\n",
       "  'chloramphenicol',\n",
       "  'kanamycin',\n",
       "  'vancomycin',\n",
       "  'rifampin',\n",
       "  'erythromycin',\n",
       "  'ciprofloxacin',\n",
       "  'staphylococcus',\n",
       "  'antimicrobial',\n",
       "  'methicillinresistant',\n",
       "  'antimicrobials',\n",
       "  'methicillinsusceptible',\n",
       "  'antimicrobialresistant'],\n",
       " ['agonist',\n",
       "  'secretory',\n",
       "  'dbdb',\n",
       "  'chemotype',\n",
       "  'pancreatic',\n",
       "  'propanoic',\n",
       "  'plasmaactive',\n",
       "  'agonists',\n",
       "  'Œ≤cells',\n",
       "  'arylsubstituted',\n",
       "  'glycemic',\n",
       "  'acidmediated',\n",
       "  'glucagonlike',\n",
       "  'glucosestimulated',\n",
       "  'gproteincoupled'],\n",
       " ['chelating',\n",
       "  'melatonin',\n",
       "  'selectivity',\n",
       "  'longterm',\n",
       "  'ferulic',\n",
       "  'deacetylase',\n",
       "  'equimolar',\n",
       "  'amidebased',\n",
       "  'antioxidant',\n",
       "  'neurotoxicity',\n",
       "  'neurodegenerative',\n",
       "  'structureactivity',\n",
       "  'multitargetdirected',\n",
       "  'immunomodulatory',\n",
       "  'diseasemodifying'],\n",
       " ['synergistically',\n",
       "  'creatinine',\n",
       "  'osteopontin',\n",
       "  'untreated',\n",
       "  'synergistic',\n",
       "  'diastole',\n",
       "  'echocardiography',\n",
       "  'cardiorenal',\n",
       "  'carvedilol',\n",
       "  'systolic',\n",
       "  'administrationapproved',\n",
       "  'carvediloltreatment',\n",
       "  'diastolenew',\n",
       "  'nonsignificant',\n",
       "  'Œ±Œ≤adrenergic'],\n",
       " ['eggs',\n",
       "  'productivity',\n",
       "  'chicken',\n",
       "  'egg',\n",
       "  'fertility',\n",
       "  'poultry',\n",
       "  'vanaraja',\n",
       "  'srinidhi',\n",
       "  'adaptable',\n",
       "  'yolk',\n",
       "  'upscaling',\n",
       "  'henday',\n",
       "  'agoclimatic',\n",
       "  'welladapted',\n",
       "  'germplasm'],\n",
       " ['phenylindole',\n",
       "  'glial',\n",
       "  'nnfresh',\n",
       "  'incubation',\n",
       "  'staining',\n",
       "  'cadaver',\n",
       "  'fibrillary',\n",
       "  'diamidino',\n",
       "  'outperformed',\n",
       "  'threedimensional',\n",
       "  'optimized',\n",
       "  'immunostaining',\n",
       "  'macrostructure',\n",
       "  'threedimensionally',\n",
       "  'xclarity'],\n",
       " ['antitumor',\n",
       "  'melanoma',\n",
       "  'transfused',\n",
       "  'chemokine',\n",
       "  'antiprogrammed',\n",
       "  'angiotensin',\n",
       "  'nninhibiting',\n",
       "  'antigenspecific',\n",
       "  'microenvironment',\n",
       "  'cancerinduced',\n",
       "  'fibroblasts',\n",
       "  'reninangiotensin',\n",
       "  'nnfibroblasts',\n",
       "  'immunosuppression',\n",
       "  'tumorinfiltrating'],\n",
       " ['toward',\n",
       "  'similarities',\n",
       "  'decades',\n",
       "  'efforts',\n",
       "  'interest',\n",
       "  'witnessed',\n",
       "  'tests',\n",
       "  'reliability',\n",
       "  'substantial',\n",
       "  'evidenced',\n",
       "  'prediction',\n",
       "  'methodologies',\n",
       "  'generalpurpose',\n",
       "  'agrochemical',\n",
       "  'applicability'],\n",
       " ['composites',\n",
       "  'gels',\n",
       "  'tendon',\n",
       "  'fragility',\n",
       "  'toughening',\n",
       "  'earlystage',\n",
       "  'hydrogels',\n",
       "  'sacrificial',\n",
       "  'elucidated',\n",
       "  'osteogenesis',\n",
       "  'biominerals',\n",
       "  'regenerative',\n",
       "  'gelbiomineral',\n",
       "  'cartilage',\n",
       "  'biomacromolecules'],\n",
       " ['aspects',\n",
       "  'phenomena',\n",
       "  'approaches',\n",
       "  'limitations',\n",
       "  'summarize',\n",
       "  'granulation',\n",
       "  'compositional',\n",
       "  'powders',\n",
       "  'flowability',\n",
       "  'agglomeration',\n",
       "  'simplifications',\n",
       "  'physicsbased',\n",
       "  'computeraided',\n",
       "  'particlescale',\n",
       "  'discreteelement']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_keywords_doc_bert=phrase_generator(docs1,tfidf_words,tfidf_countvec)\n",
    "tfidf_keywords_doc_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linezolid',\n",
       " 'fluctuating',\n",
       " 'kanamycin',\n",
       " 'tetracycline',\n",
       " 'rifampin',\n",
       " 'chloramphenicol',\n",
       " 'vancomycin',\n",
       " 'erythromycin',\n",
       " 'methicillinresistant',\n",
       " 'antimicrobial',\n",
       " 'staphylococcus',\n",
       " 'ciprofloxacin',\n",
       " 'antimicrobials',\n",
       " 'methicillinsusceptible',\n",
       " 'antimicrobialresistant']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_keywords_doc_bert[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Transformer RANKING \n",
    "'biomed_RoBERTa-base to 2.68 million scientific papers '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/biomed_roberta_base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"distilroberta-base\"\n",
    "model_name='allenai/biomed_roberta_base'\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=preprocessed_df1['lemmatize_text'].dropna().tolist()\n",
    "cand=tfidf_keywords_doc_bert[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0071, -0.2261, -0.3298,  ..., -0.1019, -0.0607, -0.0847],\n",
      "        [-0.0174, -0.2169, -0.2836,  ..., -0.0713, -0.0784, -0.1130],\n",
      "        [-0.0118, -0.2185, -0.3016,  ..., -0.1093, -0.0763, -0.0832],\n",
      "        ...,\n",
      "        [-0.0183, -0.2111, -0.3005,  ..., -0.0932, -0.0795, -0.0869],\n",
      "        [-0.0101, -0.2473, -0.3118,  ..., -0.1012, -0.0755, -0.0712],\n",
      "        [-0.0158, -0.2245, -0.3067,  ..., -0.1015, -0.0803, -0.0722]],\n",
      "       grad_fn=<TanhBackward>)\n",
      "[('methicillinresistant', 0.9887), ('staphylococcus', 0.988), ('antimicrobialresistant', 0.9882), ('methicillinsusceptible', 0.9881), ('antimicrobial', 0.988), ('rifampin', 0.9867), ('antimicrobials', 0.9871), ('chloramphenicol', 0.9865), ('ciprofloxacin', 0.9865), ('tetracycline', 0.9863)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "candidate_tokens = tokenizer(cand, padding=True, return_tensors=\"pt\")\n",
    "candidate_embeddings = model(**candidate_tokens)[\"pooler_output\"]\n",
    "print(candidate_embeddings)\n",
    "text_tokens = tokenizer([text_list[0]], padding=True, return_tensors=\"pt\")\n",
    "text_embedding = model(**text_tokens)[\"pooler_output\"]\n",
    "\n",
    "\n",
    "candidate_embeddings = candidate_embeddings.detach().numpy()\n",
    "text_embedding = text_embedding.detach().numpy()\n",
    "\n",
    "tfidf_keywords_doc=mmr(text_embedding,candidate_embeddings,tfidf_keywords_doc_bert[0],10,0.2)\n",
    "print(tfidf_keywords_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list=preprocessed_df1['lemmatize_text'].dropna().tolist()\n",
    "len(tfidf_keywords_doc_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|‚ñä                                                                                 | 1/100 [00:01<02:51,  1.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|‚ñà‚ñã                                                                                | 2/100 [00:02<02:28,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|‚ñà‚ñà‚ñç                                                                               | 3/100 [00:03<02:13,  1.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|‚ñà‚ñà‚ñà‚ñé                                                                              | 4/100 [00:05<02:14,  1.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|‚ñà‚ñà‚ñà‚ñà                                                                              | 5/100 [00:06<02:11,  1.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                             | 6/100 [00:07<02:03,  1.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                            | 7/100 [00:09<02:05,  1.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                           | 8/100 [00:09<01:42,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                          | 9/100 [00:10<01:27,  1.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                         | 10/100 [00:10<01:16,  1.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                        | 11/100 [00:11<01:10,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                       | 12/100 [00:12<01:04,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                      | 13/100 [00:13<01:06,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                     | 14/100 [00:13<01:00,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                    | 15/100 [00:14<00:57,  1.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                    | 16/100 [00:14<00:55,  1.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                   | 17/100 [00:15<00:56,  1.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                  | 18/100 [00:16<00:50,  1.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                 | 19/100 [00:17<00:58,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                | 20/100 [00:17<00:56,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                | 21/100 [00:18<01:00,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                               | 22/100 [00:19<01:01,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                              | 23/100 [00:20<00:59,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                            | 25/100 [00:21<00:51,  1.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                            | 26/100 [00:21<00:52,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                           | 27/100 [00:22<00:48,  1.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                          | 28/100 [00:23<00:54,  1.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                         | 29/100 [00:24<00:54,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                        | 31/100 [00:25<00:46,  1.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                       | 32/100 [00:25<00:47,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                      | 33/100 [00:26<00:45,  1.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                     | 34/100 [00:27<00:46,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                    | 35/100 [00:28<00:46,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                   | 36/100 [00:28<00:42,  1.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                   | 37/100 [00:29<00:48,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                  | 38/100 [00:30<00:47,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                 | 39/100 [00:31<00:50,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                | 40/100 [00:31<00:36,  1.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                               | 41/100 [00:32<00:41,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                               | 42/100 [00:33<00:42,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                              | 43/100 [00:33<00:41,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                             | 44/100 [00:34<00:44,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                            | 45/100 [00:35<00:43,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                           | 46/100 [00:36<00:47,  1.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                           | 47/100 [00:37<00:47,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                          | 48/100 [00:38<00:43,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                         | 49/100 [00:38<00:40,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                        | 50/100 [00:39<00:40,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                       | 51/100 [00:40<00:36,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                       | 52/100 [00:41<00:34,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                      | 53/100 [00:42<00:37,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                     | 54/100 [00:42<00:36,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                    | 55/100 [00:43<00:33,  1.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 56/100 [00:44<00:33,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 57/100 [00:44<00:31,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 58/100 [00:45<00:32,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 59/100 [00:46<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 60/100 [00:47<00:30,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 62/100 [00:48<00:25,  1.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 63/100 [00:48<00:24,  1.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 64/100 [00:49<00:24,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 65/100 [00:50<00:26,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 66/100 [00:51<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 67/100 [00:52<00:26,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 68/100 [00:53<00:26,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 69/100 [00:53<00:25,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 71/100 [00:54<00:19,  1.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 72/100 [00:55<00:18,  1.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 73/100 [00:56<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 74/100 [00:56<00:18,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 75/100 [00:57<00:17,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 76/100 [00:58<00:16,  1.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 77/100 [00:59<00:17,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 78/100 [01:00<00:18,  1.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 79/100 [01:01<00:21,  1.02s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 80/100 [01:02<00:19,  1.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 81/100 [01:03<00:18,  1.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 82/100 [01:04<00:15,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 83/100 [01:05<00:16,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 84/100 [01:06<00:16,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 85/100 [01:06<00:13,  1.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 86/100 [01:07<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 87/100 [01:08<00:09,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 88/100 [01:08<00:08,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 89/100 [01:09<00:08,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 90/100 [01:11<00:09,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 91/100 [01:11<00:08,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 92/100 [01:12<00:07,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 93/100 [01:13<00:05,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 94/100 [01:14<00:04,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 95/100 [01:15<00:04,  1.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 96/100 [01:16<00:03,  1.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 97/100 [01:16<00:02,  1.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 98/100 [01:17<00:01,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 99/100 [01:18<00:00,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:18<00:00,  1.27it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "tfidf_keywords_pre_bert=[]\n",
    "\n",
    "for index, doc in enumerate(tqdm(docs1[:100])):\n",
    "    candidate_tokens = tokenizer(tfidf_keywords_doc_bert[index], padding=True, return_tensors=\"pt\",truncation=True)\n",
    "    \n",
    "    candidate_embeddings = model(**candidate_tokens)[\"pooler_output\"]\n",
    "    \n",
    "    candidate_embeddings = candidate_embeddings.detach().numpy()\n",
    "    \n",
    "    text_tokens = tokenizer([text_list[index]], padding=True, return_tensors=\"pt\",truncation=True)\n",
    "    text_embedding = model(**text_tokens)[\"pooler_output\"]\n",
    "    text_embedding = text_embedding.detach().numpy()\n",
    "    \n",
    "    tfidf_keywords_doc_mmr=mmr(text_embedding,candidate_embeddings,tfidf_keywords_doc_bert[index],15,0.2)\n",
    "    \n",
    "    tfidf_keywords_pre_bert.append(tfidf_keywords_doc_mmr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('antimicrobial', 0.9893),\n",
       " ('staphylococcus', 0.9888),\n",
       " ('methicillin', 0.9887),\n",
       " ('antimicrobials', 0.9887),\n",
       " ('isolates', 0.9878),\n",
       " ('penicillin', 0.988),\n",
       " ('linezolid', 0.9873),\n",
       " ('ciprofloxacin', 0.9874),\n",
       " ('tetracycline', 0.9874),\n",
       " ('chloramphenicol', 0.9873),\n",
       " ('kanamycin', 0.9871),\n",
       " ('trimethoprim', 0.9871),\n",
       " ('erythromycin', 0.9869),\n",
       " ('vancomycin', 0.9868),\n",
       " ('fluctuating', 0.9856)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_keywords_pre_bert[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximal_marginal_relevance(sentence_vector, phrases, embedding_matrix, lambda_constant=0.5, threshold_terms=10):\n",
    "    \"\"\"\n",
    "    Return ranked phrases using MMR. Cosine similarity is used as similarity measure.\n",
    "    :param sentence_vector: Query vector\n",
    "    :param phrases: list of candidate phrases\n",
    "    :param embedding_matrix: matrix having index as phrases and values as vector\n",
    "    :param lambda_constant: 0.5 to balance diversity and accuracy. if lambda_constant is high, then higher accuracy. If lambda_constant is low then high diversity.\n",
    "    :param threshold_terms: number of terms to include in result set\n",
    "    :return: Ranked phrases with score\n",
    "    \"\"\"\n",
    "    # todo: Use cosine similarity matrix for lookup among phrases instead of making call everytime.\n",
    "    s = []\n",
    "    r = sorted(phrases, key=lambda x: x[1], reverse=True)\n",
    "    r = [i[0] for i in r]\n",
    "    while len(r) > 0:\n",
    "        score = 0\n",
    "        phrase_to_add = ''\n",
    "        for i in r:\n",
    "            first_part = cosine_similarity([sentence_vector], [embedding_matrix.loc[i]])[0][0]\n",
    "            second_part = 0\n",
    "            for j in s:\n",
    "                cos_sim = cosine_similarity([embedding_matrix.loc[i]], [embedding_matrix.loc[j[0]]])[0][0]\n",
    "                if cos_sim > second_part:\n",
    "                    second_part = cos_sim\n",
    "            equation_score = lambda_constant*(first_part)-(1-lambda_constant) * second_part\n",
    "            if equation_score > score:\n",
    "                score = equation_score\n",
    "                phrase_to_add = i\n",
    "        if phrase_to_add == '':\n",
    "            phrase_to_add = i\n",
    "        r.remove(phrase_to_add)\n",
    "        s.append((phrase_to_add, score))\n",
    "    return (s, s[:threshold_terms])[threshold_terms > len(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_generator(docs,words):\n",
    "    \n",
    "    top_n:int=15\n",
    "    diversity:0.5\n",
    "    \n",
    "    keywords=[]    \n",
    "    \n",
    "    \n",
    "    for index, doc in enumerate(tqdm(docs)):\n",
    "        doc_words = [words[i] for i in df[index].nonzero()[1]]\n",
    "        if doc_words:\n",
    "            doc_word_embeddings = np.array([tfidf_word_embeddings_bert[i] for i in df[index].nonzero()[1]])\n",
    "            distances = cosine_similarity([tfidf_doc_embeddings_bert[index]], doc_word_embeddings)[0]\n",
    "            doc_keywords = [(doc_words[i], round(float(distances[i]), 4)) for i in distances.argsort()[-top_n:]]\n",
    "            doc_keywords=[key[0] for key in doc_keywords]\n",
    "            keywords.append(doc_keywords)\n",
    "#             phrases.append(doc_phrases)                                 \n",
    "    return keywords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
