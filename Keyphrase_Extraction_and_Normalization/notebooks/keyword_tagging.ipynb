{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fasttext_win\n",
    "# !pip install sentencepiece\n",
    "#Importing libraries\n",
    "\n",
    "import re,string\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "# import fasttext\n",
    "import spacy\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet,stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import WordNetLemmatizer, pos_tag, word_tokenize\n",
    "\n",
    "\n",
    "stop_words=stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Activation, Embedding, Reshape,Concatenate\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import skipgrams, make_sampling_table\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       contamination of meat with antimicrobial-resis...\n",
       "1       g-protein-coupled receptor 40 (gpr40) is consi...\n",
       "2       the structures of melatonin and ferulic acid w...\n",
       "3       there are currently no food and drug administr...\n",
       "4       productivity of traditional backyard poultry i...\n",
       "                              ...                        \n",
       "4995    nutritional risk screening (nrs) is not yet es...\n",
       "4996    purpose: a growing number of studies indicate ...\n",
       "4997    non-alcoholic-fatty liver disease (nafld) is s...\n",
       "4998    we propose and test a model of food policy acc...\n",
       "4999    this study aimed to analyze the physicochemica...\n",
       "Name: Abstract, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sample_data=pd.read_excel('../data/recent_5k.xlsx')\n",
    "sample_abs_data=sample_data['Abstract']\n",
    "sample_abs_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    cleaned_txt = re.sub('<[^<]+>', ' ', str(text))\n",
    "    cleaned_txt = re.sub( r'\\[.*?\\]', ' ',cleaned_txt)  #remove brackets\n",
    "    cleaned_txt = re.sub('\\d+', ' ', cleaned_txt)\n",
    "    cleaned_txt = re.sub('\\(\\w+\\)', ' ', cleaned_txt)\n",
    "#     cleaned_txt=text.translate(str.maketrans(' ', ' ',string.punctuation)) \n",
    "    return cleaned_txt\n",
    "   \n",
    "def stopword_removal(text):\n",
    "    cleaned_txt = ' '.join([token for token in word_tokenize(text) if token not in stop_words])\n",
    "    return cleaned_txt\n",
    "def lemmatize_text(text):\n",
    "    cleaned_txt=' '.join([lemmatizer.lemmatize(token) for token in word_tokenize(text)])\n",
    "\n",
    "    return cleaned_txt\n",
    "\n",
    "def preprocessed_text(df):\n",
    "    \n",
    "    col=['nltk_tokens','nouns_verbs','lemmatize_text']\n",
    "    preprocess_txt=pd.DataFrame(columns=col)\n",
    "    abs_clean_text=df.apply(text_cleaning)\n",
    "    \n",
    "    stopword_removed_text=abs_clean_text.apply(stopword_removal)         \n",
    "    preprocess_txt['lemmatize_text']=stopword_removed_text.apply(lemmatize_text)\n",
    "  \n",
    "    for ind,text in preprocess_txt['lemmatize_text'].iteritems():       \n",
    "        tokens=word_tokenize(text)\n",
    "        pos_tokens=nltk.pos_tag(tokens)\n",
    "        preprocess_txt.at[ind,'nouns_verbs']=[pos_token for pos_token,pos in pos_tokens if pos.startswith('N' or 'V')]\n",
    "        preprocess_txt.at[ind,'nltk_tokens']=tokens     \n",
    "    return preprocess_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nltk_tokens</th>\n",
       "      <th>nouns_verbs</th>\n",
       "      <th>lemmatize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[contamination, meat, antimicrobial-resistant,...</td>\n",
       "      <td>[contamination, meat, bacteria, health, threat...</td>\n",
       "      <td>contamination meat antimicrobial-resistant bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[g-protein-coupled, receptor, (, gpr, ), consi...</td>\n",
       "      <td>[receptor, gpr, drug, target, diabetes\\, role,...</td>\n",
       "      <td>g-protein-coupled receptor ( gpr ) considered ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[structure, melatonin, ferulic, acid, merged, ...</td>\n",
       "      <td>[structure, acid, histone, deacetylase, hdac, ...</td>\n",
       "      <td>structure melatonin ferulic acid merged tertia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[currently, food, drug, administration-approve...</td>\n",
       "      <td>[food, drug, treatment, heart, failure, ejecti...</td>\n",
       "      <td>currently food drug administration-approved tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[productivity, traditional, backyard, poultry,...</td>\n",
       "      <td>[productivity, backyard, poultry, country, int...</td>\n",
       "      <td>productivity traditional backyard poultry deve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>[nutritional, risk, screening, yet, establishe...</td>\n",
       "      <td>[risk, screening, setting, study, efficacy, to...</td>\n",
       "      <td>nutritional risk screening yet established man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>[purpose, :, growing, number, study, indicate,...</td>\n",
       "      <td>[purpose, number, study, importance, vitamin, ...</td>\n",
       "      <td>purpose : growing number study indicate import...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>[non-alcoholic-fatty, liver, disease, spreadin...</td>\n",
       "      <td>[liver, disease, worldwide, drug, nafld, plant...</td>\n",
       "      <td>non-alcoholic-fatty liver disease spreading wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>[propose, test, model, food, policy, acceptabi...</td>\n",
       "      <td>[test, model, food, policy, acceptability, mod...</td>\n",
       "      <td>propose test model food policy acceptability ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>[study, aimed, analyze, physicochemical, chara...</td>\n",
       "      <td>[study, effect, pulp, fruit, consumption\\, sti...</td>\n",
       "      <td>study aimed analyze physicochemical characteri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            nltk_tokens  \\\n",
       "0     [contamination, meat, antimicrobial-resistant,...   \n",
       "1     [g-protein-coupled, receptor, (, gpr, ), consi...   \n",
       "2     [structure, melatonin, ferulic, acid, merged, ...   \n",
       "3     [currently, food, drug, administration-approve...   \n",
       "4     [productivity, traditional, backyard, poultry,...   \n",
       "...                                                 ...   \n",
       "4995  [nutritional, risk, screening, yet, establishe...   \n",
       "4996  [purpose, :, growing, number, study, indicate,...   \n",
       "4997  [non-alcoholic-fatty, liver, disease, spreadin...   \n",
       "4998  [propose, test, model, food, policy, acceptabi...   \n",
       "4999  [study, aimed, analyze, physicochemical, chara...   \n",
       "\n",
       "                                            nouns_verbs  \\\n",
       "0     [contamination, meat, bacteria, health, threat...   \n",
       "1     [receptor, gpr, drug, target, diabetes\\, role,...   \n",
       "2     [structure, acid, histone, deacetylase, hdac, ...   \n",
       "3     [food, drug, treatment, heart, failure, ejecti...   \n",
       "4     [productivity, backyard, poultry, country, int...   \n",
       "...                                                 ...   \n",
       "4995  [risk, screening, setting, study, efficacy, to...   \n",
       "4996  [purpose, number, study, importance, vitamin, ...   \n",
       "4997  [liver, disease, worldwide, drug, nafld, plant...   \n",
       "4998  [test, model, food, policy, acceptability, mod...   \n",
       "4999  [study, effect, pulp, fruit, consumption\\, sti...   \n",
       "\n",
       "                                         lemmatize_text  \n",
       "0     contamination meat antimicrobial-resistant bac...  \n",
       "1     g-protein-coupled receptor ( gpr ) considered ...  \n",
       "2     structure melatonin ferulic acid merged tertia...  \n",
       "3     currently food drug administration-approved tr...  \n",
       "4     productivity traditional backyard poultry deve...  \n",
       "...                                                 ...  \n",
       "4995  nutritional risk screening yet established man...  \n",
       "4996  purpose : growing number study indicate import...  \n",
       "4997  non-alcoholic-fatty liver disease spreading wo...  \n",
       "4998  propose test model food policy acceptability ....  \n",
       "4999  study aimed analyze physicochemical characteri...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df1=preprocessed_text(sample_abs_data)\n",
    "preprocessed_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sequence data  generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_VOCAB_SIZE=2000\n",
    "# # Create a tokenizer for the input texts and fit it to them \n",
    "# tokenizer_inputs = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n",
    "# tokenizer_inputs.fit_on_texts(sents)\n",
    "# # Tokenize and transform input texts to sequence of integers\n",
    "# input_sequences = tokenizer_inputs.texts_to_sequences(sents)\n",
    "# # Claculate the max length\n",
    "# input_max_len = max(len(s) for s in input_sequences)\n",
    "# print('Max Input Length: ', input_max_len)\n",
    "# # Show some example of tokenize sentences, useful to check the tokenization\n",
    "# # print(sentences[100])\n",
    "# # print(input_sequences[100])\n",
    "\n",
    "# # get the word to index mapping for input language\n",
    "# word2idx_inputs = tokenizer_inputs.word_index\n",
    "# print('Found %s unique input tokens.' % len(word2idx_inputs))\n",
    "\n",
    "# print(word2idx_inputs)\n",
    "\n",
    "# # store number of output and input words for later\n",
    "# # remember to add 1 since indexing starts at 1\n",
    "# num_words_inputs = len(word2idx_inputs) + 1\n",
    "\n",
    "# # map indexes back into real words\n",
    "# # so we can view the results\n",
    "# idx2word_inputs = {v:k for k, v in word2idx_inputs.items()}\n",
    "# # pad the input sequences\n",
    "# encoder_inputs = pad_sequences(input_sequences, maxlen=input_max_len, padding='post')\n",
    "# print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n",
    "# print(\"encoder_inputs[0]:\", encoder_inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastext Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protein coupled receptor gpr40 considered attractive drug target treating type diabetes owing role free fatty acid mediated increase glucose stimulated insulin secretion gsis pancreatic cell identify chemotype gpr40 agonist series aryl substituted indole propanoic acid derivative designed synthesized identified gpr40 agonist lead compound fluoro methylphenyl indol propanoic acid dimethylphenyl indol propanoic acid gsis glucagon like peptide secretory effect unlike previously reported gpr40 partial agonist activate pathway activated signaling pathway characterized gpr40 full agonist vivo efficacy study significantly improved glycemic control c57bl mouse increased plasma active c57bl mouse thus represents promising lead development novel gpr40 full agonist type diabetes\n",
      "['detailed', 'analysis', 'textural', 'property', 'pore', 'size', 'connectivity', 'nanoporous', 'material', 'essential', 'identify', 'correlation', 'property', 'performance', 'storage', 'separation', 'catalysis', 'process', 'advance', 'developing', 'nanoporous', 'material', 'uniform', 'tailor', 'made', 'pore', 'structure', 'including', 'introduction', 'hierarchical', 'pore', 'system', 'offer', 'huge', 'potential', 'application', 'within', 'context', 'major', 'progress', 'made', 'understanding', 'adsorption', 'phase', 'behavior', 'confined', 'fluid', 'consequently', 'physisorption', 'characterization', 'enables', 'reliable', 'pore', 'size', 'volume', 'network', 'connectivity', 'analysis', 'using', 'advanced', 'high', 'resolution', 'experimental', 'protocol', 'coupled', 'advanced', 'method', 'based', 'statistical', 'mechanic', 'method', 'based', 'density', 'functional', 'theory', 'molecular', 'simulation', 'macro', 'pore', 'present', 'combination', 'adsorption', 'mercury', 'porosimetry', 'useful', 'hence', 'important', 'recent', 'advance', 'understanding', 'mercury', 'intrusion', 'extrusion', 'mechanism', 'discussed', 'additionally', 'promising', 'complementary', 'technique', 'characterization', 'porous', 'material', 'immersed', 'liquid', 'phase', 'introduced']\n",
      "0.9355176\n",
      "[('cas9', 0.9131219387054443), ('editing', 0.7318271398544312), ('genomics', 0.7175068259239197), ('genomic', 0.6754693984985352), ('transcriptomics', 0.6730930209159851), ('redundant', 0.6660775542259216), ('genome', 0.6651231050491333), ('allelic', 0.6488426327705383), ('translational', 0.6488422155380249), ('novel', 0.6463779807090759)]\n",
      "protein coupled receptor gpr40 considered attractive drug target treating type diabetes owing role free fatty acid mediated increase glucose stimulated insulin secretion gsis pancreatic cell identify chemotype gpr40 agonist series aryl substituted indole propanoic acid derivative designed synthesized identified gpr40 agonist lead compound fluoro methylphenyl indol propanoic acid dimethylphenyl indol propanoic acid gsis glucagon like peptide secretory effect unlike previously reported gpr40 partial agonist activate pathway activated signaling pathway characterized gpr40 full agonist vivo efficacy study significantly improved glycemic control c57bl mouse increased plasma active c57bl mouse thus represents promising lead development novel gpr40 full agonist type diabetes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD2vec text voc len 0\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "import pandas as pd \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet,stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import WordNetLemmatizer, pos_tag, word_tokenize\n",
    "\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "stop_words=stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(document):\n",
    "        # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    "\n",
    "        # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        tokens = document.split()\n",
    "        tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        tokens = [word for word in tokens if len(word) > 3]\n",
    "\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "        return preprocessed_text\n",
    "\n",
    "def word_corpus_builder(sents_list):\n",
    "    final_corpus = [preprocess_text(sentence) for sentence in sents_list if str(sentence).strip() !='']\n",
    "    print(final_corpus[1])\n",
    "    word_punctuation_tokenizer = nltk.WordPunctTokenizer()\n",
    "    word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in final_corpus]\n",
    "    print(word_tokenized_corpus[10])\n",
    "    return word_tokenized_corpus\n",
    "\n",
    "\n",
    "def fasttext_model(word_tokenized_corpus):\n",
    "    embedding_size = 60\n",
    "    window_size = 40\n",
    "    min_word = 5\n",
    "    down_sampling = 1e-2\n",
    "\n",
    "    ft_model = FastText(word_tokenized_corpus,\n",
    "                      vector_size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      sg=1)\n",
    "\n",
    "    ft_model.build_vocab(word_tokenized_corpus)\n",
    "    total_words = ft_model.corpus_total_words\n",
    "    ft_model.train(word_tokenized_corpus, total_words=total_words, epochs=5)\n",
    "    print(ft_model.wv.similarity('crispr','crisprcas'))\n",
    "    print(ft_model.wv.similar_by_word('crispr'))\n",
    "    return ft_model\n",
    "\n",
    "\n",
    "def get_feature_vec_fast(text, model):\n",
    "    # Index2word is a list that contains the names of the words in\n",
    "    # the model's vocabulary.\n",
    "    clean_text = []\n",
    "    index2word_set=set(model.wv.index_to_key) \n",
    "    words=set(text.split()) \n",
    "    \n",
    "    for word in tqdm(words):\n",
    "        if word in index2word_set:\n",
    "            if word:\n",
    "                try:\n",
    "                    clean_text.append(model[word])\n",
    "                except:\n",
    "                    pass\n",
    "    print('WORD2vec text voc len',len(clean_text))\n",
    "    return clean_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_data=pd.read_excel('../data/recent_5k.xlsx')\n",
    "    sample_abs_data=sample_data['Abstract']\n",
    "    sents_list=sample_abs_data.to_list()\n",
    "    \n",
    "    word_tokenized_corpus=word_corpus_builder(sents_list)\n",
    "    ft_model=fasttext_model(word_tokenized_corpus)\n",
    "    final_corpus = [preprocess_text(sentence) for sentence in sents_list if str(sentence).strip() !='']\n",
    "    print(final_corpus[1])\n",
    "    for i in final_corpus:\n",
    "        trainDataVecs_fast = get_feature_vec_fast(i,ft_model)\n",
    "        print(trainDataVecs_fast)\n",
    "        break\n",
    "\n",
    "    # ft_model.save('path_of_pretrained_wieights_loading_dir/word2vec5k.bin')\n",
    "    # model=Word2Vec.load('path_of_pretrained_wieights_loading_dir/word2vec5k.bin')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "import sentencepiece as spm\n",
    "import fasttext\n",
    "import numpy as np\n",
    "from fasttext import skipgram\n",
    "\n",
    "def text_cleaning(text):\n",
    "    cleaned_txt = re.sub('<[^<]+>', ' ', str(text))\n",
    "    cleaned_txt = re.sub( r'\\[.*?\\]', ' ',cleaned_txt)  #remove brackets\n",
    "    cleaned_txt = re.sub('\\d+', ' ', cleaned_txt)\n",
    "    cleaned_txt = re.sub('\\(\\w+\\)', ' ', cleaned_txt)\n",
    "#     cleaned_txt=text.translate(str.maketrans(' ', ' ',string.punctuation)) \n",
    "    return cleaned_txt\n",
    "   \n",
    "def stopword_removal(text):\n",
    "    cleaned_txt = ' '.join([token for token in word_tokenize(text) if token not in stop_words])\n",
    "    return cleaned_txt\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    cleaned_txt=' '.join([lemmatizer.lemmatize(token) for token in word_tokenize(text)])\n",
    "    return cleaned_txt\n",
    "\n",
    "def preprocessed_text(df):\n",
    "    \n",
    "    col=['nltk_tokens','nouns_verbs','lemmatize_text']\n",
    "    preprocess_txt=pd.DataFrame(columns=col)\n",
    "    abs_clean_text=df.apply(text_cleaning)\n",
    "    \n",
    "    stopword_removed_text=abs_clean_text.apply(stopword_removal)         \n",
    "    preprocess_txt['lemmatize_text']=stopword_removed_text.apply(lemmatize_text)\n",
    "  \n",
    "#     for ind,text in preprocess_txt['lemmatize_text'].iteritems():       \n",
    "#         tokens=word_tokenize(text)\n",
    "#         pos_tokens=nltk.pos_tag(tokens)\n",
    "#         preprocess_txt.at[ind,'nouns_verbs']=[pos_token for pos_token,pos in pos_tokens if pos.startswith('N' or 'V')]\n",
    "#         preprocess_txt.at[ind,'nltk_tokens']=tokens     \n",
    "    return preprocess_txt\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessed_df1=preprocessed_text(sample_abs_data)\n",
    "    sample_pat_data='protein coupled receptor gpr40 considered attractive drug target treating type diabetes owing role free fatty acid mediated increase glucose stimulated insulin secretion gsis pancreatic cell identify chemotype gpr40 agonist series aryl substituted indole propanoic acid derivative designed synthesized identified gpr40 agonist lead compound fluoro methylphenyl indol propanoic acid dimethylphenyl indol propanoic acid gsis glucagon like peptide secretory effect unlike previously reported gpr40 partial agonist activate pathway activated signaling pathway characterized gpr40 full agonist vivo efficacy study significantly improved glycemic control c57bl mouse increased plasma active c57bl mouse thus represents promising lead development novel gpr40 full agonist type diabetes'\n",
    "\n",
    "    sents=preprocessed_df1['lemmatize_text']\n",
    "    sent_list=preprocessed_df1['lemmatize_text'].tolist()\n",
    "\n",
    "    corpus_file=''.join(i for i in sents)\n",
    "    model=FastText(vector_size=100,sg=1)\n",
    "    model.build_vocab(sample_pat_data)\n",
    "    totalwords=model.corpus_total_words\n",
    "    model.train(sample_pat_data,total_words=totalwords,epochs=5)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet,stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import WordNetLemmatizer, pos_tag, word_tokenize\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "pat_data=pd.read_excel('../data/recent_5k.xlsx')\n",
    "abs_text=pat_data['Abstract']\n",
    "abs_list=pat_data['Abstract'].tolist()\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "#Tokenizing with simple preprocess gensim's simple preprocess\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(simple_preprocess(str(sentence), deacc=True)) # returns lowercase tokens, ignoring tokens that are too short or too long\n",
    "\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    filtered_words = [word for word in sentence if word not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "        \n",
    "abs_words = list(sent_to_words(abs_list))\n",
    "lengths = [len(abst) for abst in abs_words]\n",
    "plt.hist(lengths, bins = 25)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "filtered_abst = [remove_stopwords(abst) for abst in abs_words]\n",
    "lengths = [len(abst) for abst in filtered_abst]\n",
    "plt.hist(lengths, bins = 25)\n",
    "plt.show()\n",
    "\n",
    "print('Mean word count of abs is %s' % np.mean(lengths))\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "n = 50\n",
    "ft_model = FastText(filtered_abst, vector_size=n, window=8, min_count=5, workers=2,sg=1)\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "\n",
    "#To proprely work with scikit's vectorizer\n",
    "merged_questions = [' '.join(question) for question in filtered_abst]\n",
    "document_names = ['Doc {:d}'.format(i) for i in range(len(merged_questions))]\n",
    "\n",
    "def get_tfidf(docs, ngram_range=(1,1), index=None):\n",
    "    vect = TfidfVectorizer(stop_words='english', ngram_range=ngram_range)\n",
    "    tfidf = vect.fit_transform(docs).todense()\n",
    "    return pd.DataFrame(tfidf, columns=vect.get_feature_names(), index=index).T\n",
    "\n",
    "tfidf = get_tfidf(merged_questions, ngram_range=(1,1), index=document_names)\n",
    "\n",
    "def get_sent_embs(emb_model):\n",
    "    sent_embs = []\n",
    "    for desc in range(len(filtered_abst)):\n",
    "        sent_emb = np.zeros((1, n))\n",
    "        if len(filtered_abst[desc]) > 0:\n",
    "            sent_emb = np.zeros((1, n))\n",
    "            div = 0\n",
    "            model = emb_model\n",
    "            for word in filtered_abst[desc]:\n",
    "                if word in model.wv.key_to_index and  word in tfidf.index:\n",
    "                    word_emb = model.wv[word]\n",
    "                    weight = tfidf.loc[word, 'Doc {:d}'.format(desc)]\n",
    "                    \n",
    "                    sent_emb = np.add(sent_emb, word_emb * weight)\n",
    "                    div += weight\n",
    "                else:\n",
    "                    div += 1e-13 #to avoid dividing by 0\n",
    "        if div == 0:\n",
    "            print(desc)\n",
    "\n",
    "        sent_emb = np.divide(sent_emb, div)\n",
    "        sent_embs.append(sent_emb.flatten())\n",
    "    return sent_embs\n",
    "\n",
    "ft_sent = get_sent_embs(emb_model = ft_model) \n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "def get_n_most_similar(interest_index, embeddings, n):\n",
    "    \"\"\"\n",
    "    Takes the embedding vector of interest, the list with all embeddings, and the number of similar abstract to \n",
    "    retrieve.\n",
    "    Outputs the disctionary IDs and distances\n",
    "    \"\"\"\n",
    "    nbrs = NearestNeighbors(n_neighbors=n, metric='cosine').fit(embeddings)\n",
    "    distances, indices = nbrs.kneighbors(embeddings)\n",
    "    similar_indices = indices[interest_index][1:]\n",
    "    similar_distances = distances[interest_index][1:]\n",
    "    return similar_indices, similar_distances\n",
    "\n",
    "def print_similar(interest_index, embeddings, n):\n",
    "    \"\"\"\n",
    "    Convenience function for visual analysis\n",
    "    \"\"\"\n",
    "    closest_ind, closest_dist = get_n_most_similar(interest_index, embeddings, n)\n",
    "    print('Abstract %s \\n \\n is most similar to these %s abstract: \\n' % (abs_list[interest_index], n))\n",
    "    for question in closest_ind:\n",
    "        print('ID ', question, ': ',abs_list[question])\n",
    "        \n",
    "print_similar(42, ft_sent, 5)  \n",
    "\n",
    "#################################################################################\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(filtered_abst)]\n",
    "model = Doc2Vec(documents, vector_size=n, window=8, min_count=5, workers=2, dm = 1, epochs=20)\n",
    "print(abs_list[42], ' \\nis similar to \\n')\n",
    "print([abs_list[similar[0]] for similar in model.docvecs.most_similar(42)])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"D001151\\tChemical\\tArsenic\\tMechanisms Underlying Latent Disease Risk Associated with Early-Life <target> Arsenic </target> Exposure : Current Research Trends and Scientific Gaps .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D001151\\tChemical\\tArsenic\\tMechanisms Underlying Latent Disease Risk Associated with Early-Life <target> Arsenic <',\n",
       " 'arget> Exposure : Current Research Trends and Scientific Gaps .']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split('/t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('path_of_file.txt',delimiter='/t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
